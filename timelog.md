# Timelog

* Developing an Estimator for the Environmental Footprint of Big Data Jobs 
* James Sharma
* 2469702s
* Supervised by Lauritz Thamsen

## Guidance


## Week 1

* *No work done as project allocations not released until Wednesday 28th September*

## Week 2

### 28 September 

* *2 hours* Scheduling meeting with supervisor, and first pass at relevant papers 

### 29 September 

* *4 hours* Reading the project guidance notes and attending the information lecture 

### 30 September

* *0.5 hours* meeting with supervisor
* *0.5 hour* Created GitLab repository and cloned the cookiecutter for the projects
* *2 hours* Watched recording of Lauritz's and Wim's low carbon seminars

### 1 October 

* *2 hours* Read Bell paper "Selecting Resources for Distributed Dataflow..." by Thamsen 

## Week 3

### 5 October

* *1 hour* Created GitLab repository and set up the cookiecutter for the projects
* *1 hour* Populated timesheet with information 
* *0.5 hour* meeting with supervisor


### 7 October 

* *1.5 hours* looked at C3O paper "Collaborative Cluster Configuration Optimization..." by Thamsen 
* *1.5 hours* looked at Bellamy paper "Reusing Performance Models..." by Thamsen 

### 9 October 

* *1 hour* studied Chapter 1 of cloud course sides
* *2 hours* studied Chapter 2 of cloud course slides 
* *2 hours* studied chapter 5 of cloud course slides

## Week 4

### 11 October

* *0.5 hours* looked at Teads blog "Evaluation the carbon footprint of..." 
* *0.5 hours* looked at Teads blog "Estimated AWS EC2 Instances..."

### 12 October

* *1 hour* looked at Teads blog "Building an AWC EC2 Emissions Dataset"
* *0.5 hours* meeting with supervisor


### 14 October

* *1 hour* looked at scout and teads datasets

### Rest of week 4

* *No work done due to illness*

## Week 5

* *No work done due to illness*

## Week 6

### 24 October

* *0.5 hours* pulled and pushed scout and teads data

* *1 hour* processed teads data to make new CSV with only relevant information

* *2 hours* processed scout data to create new CSVs with average utilisations for each run

* *2 hours* processed scout data to make new CSVs with only relevant 5-second utilisation information

* *1 hour* updated new CSVs to combine identical runs with same job and instance type

* *0.5 hours* tidied up repository structure, commented and formatted code

### 26 October

* *1 hour* looked into power curve fitting 

* *0.5 hours* updating timesheet and sending email

* *0.5 hours* meeting with supervisor

## Week 7

### 1 November

* *1.5 hours* looking into Cloud Carbon Footprint methodology and appendicies 

* *2 hours* creading CSV containing cpu average of all multinode data

### 2 November

* *2 hours* data analysis to determine effect of scale out behaviour on utilisation metric

* *1 hour* attempted to fit power curves for each instance type 

* *0.5 hours* meeting with supervisor 

## Week 8

### 8 November

* *1 hour* looked more closely at power curve statistical metrics 

* *2 hours* read MSci Dissertation on estimating carbon

* *1 hour* looked at MSci Dissertation GitHub spreadsheet and notebook

### 9 November

* *2 hours* applied power curves for utilisation metric to compute estimated power

* *1 hour* used estimated power to compute estimated carbon emissions 

* *0.5 hours* meeting with supervisor 

## Week 9 

### 15 November

* *2 hours* looked into similar cloud carbon footprint estimators for design ideas

### 16 November 

* *1 hour* Setting up streamlit infrastructure 

* *1.5 hours* Calculator settings frontend design

* *1 hour* Calculate button functionality 

* *0.5 hours* Loading in data and plotting chart for analysis page

* *1 hour* restructured file input and default input, using session state 

* *0.5 hours* meeting with supervisor

## Week 10

### 21 November

* *3 hours* applied feedback as suggested in previous meeting 

### 23 November

* *4 hours* implemented simple prototype calculating and fitting scale out based on linear interpolation

* *0.5 hours* meeting with supervisor

## Week 11

* *No work done due to several overlapping coursework deadlines*

## Week 12

### 28 November

* *0.5 hours* reread bell paper

* *2 hours* thoroughly read C30 paper


### 30 November

* *1 hour* looked into project summary status report

* *0.5 hours* looked into gradient boosting classification

* *2 hours* looked into gradient boosting regression

* *1 hour* attempted to implement GBR, unsuccessful

* *0.5 hours* meeting with supervisor 

## Week 13

### 5 December

* *2 hours* successfully implemented GBR for node count on scout dataset

* *1 hour* unsuccessfully attempted to implement cross validation to test GBR result 

### Rest of week 13

* *No futher work done due to PSI exam on the 9th 

## Week 14

### 12 December

* *3 hours* read dissertation writing and evaluation guidance slides


### 13 December

* *2 hours* read powerpoint on scientific text writing

### 14 December

* *2 hours* wrote up project status report 

* *3 hours* wrote up draft abstract and started introduction

## Week 15 

### 21 December

* *3 hours* finished state-of-the-art and approach section in draft introduction

* *2 hours* attempted cross validation using different library, unsuccessful

### 22 December

* *1 hour* finishing touches on draft introduction

* *0.5 hours* project meeting with supervisor 

### Rest of week 15

* *No futher work done due to Christmas Break

## Week 16

* *No work done due to New Year Break

## Week 17

### 3 January

* *1 hour* refamiliarised self with project 

* *1 hour* updated timesheets according to meeting notes

* *0.5 hours* updated project status report based on feedback

### 4 January

* *0.5 hours* improved documentation of existing notebook files

* *3 hours* created working script modelling runtime and cpu utilisation given available features

### 5 January

* *2 hours* script now produces csv containing predictions and recovered feature labels 

* *2 hours* calculator options now dynamically generate

* *3 hours* implemented script into existing streamlit webapp

### 6 January

* *1 hour* tidied up streamlit file