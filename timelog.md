# Timelog

* Developing an Estimator for the Environmental Footprint of Big Data Jobs 
* James Sharma
* 2469702s
* Supervised by Lauritz Thamsen

## Guidance


## Week 1

* *No work done as project allocations not released until Wednesday 28th September*

## Week 2

### 28 September 

* *2 hours* Scheduling meeting with supervisor, and first pass at relevant papers 

### 29 September 

* *4 hours* Reading the project guidance notes and attending the information lecture 

### 30 September

* *0.5 hours* meeting with supervisor
* *0.5 hour* Created GitLab repository and cloned the cookiecutter for the projects
* *2 hours* Watched recording of Lauritz's and Wim's low carbon seminars

### 1 October 

* *2 hours* Read Bell paper "Selecting Resources for Distributed Dataflow..." by Thamsen 

## Week 3

### 5 October

* *1 hour* Created GitLab repository and set up the cookiecutter for the projects
* *1 hour* Populated timesheet with information 
* *0.5 hour* meeting with supervisor


### 7 October 

* *1.5 hours* looked at C3O paper "Collaborative Cluster Configuration Optimization..." by Thamsen 
* *1.5 hours* looked at Bellamy paper "Reusing Performance Models..." by Thamsen 

### 9 October 

* *1 hour* studied Chapter 1 of cloud course sides
* *2 hours* studied Chapter 2 of cloud course slides 
* *2 hours* studied chapter 5 of cloud course slides

## Week 4

### 11 October

* *0.5 hours* looked at Teads blog "Evaluation the carbon footprint of..." 
* *0.5 hours* looked at Teads blog "Estimated AWS EC2 Instances..."

### 12 October

* *1 hour* looked at Teads blog "Building an AWC EC2 Emissions Dataset"
* *0.5 hours* meeting with supervisor


### 14 October

* *1 hour* looked at scout and teads datasets

### Rest of week 4

* *No work done due to illness*

## Week 5

* *No work done due to illness*

## Week 6

## 24 October

* *0.5 hours* pulled and pushed scout and teads data

* *1 hour* processed teads data to make new CSV with only relevant information

* *2 hours* processed scout data to create new CSVs with average utilisations for each run

* *2 hours* processed scout data to make new CSVs with only relevant 5-second utilisation information

* *1 hour* updated new CSVs to combine identical runs with same job and instance type

* *0.5 hours* tidied up repository structure, commented and formatted code

## 26 October

* *1 hour* looked into power curve fitting 

* *0.5 hours* updating timesheet and sending email

* *0.5 hours* meeting with supervisor

## 1 November

* *1.5 hours* looking into Cloud Carbon Footprint methodology and appendicies 

* *2 hours* creading CSV containing cpu average of all multinode data

# 2 November

* *2 hours* data analysis to determine effect of scale out behaviour on utilisation metric

* *1 hour* attempted to fit power curves for each instance type 

* *0.5 hours* meeting with supervisor 